{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from unicodedata import normalize\n",
    "import numpy as np\n",
    "import keras\n",
    "import keras.utils as ku \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/joey/Desktop/C/CS584/assignment4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    # open and read file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def to_sentence(doc):\n",
    "    # @para: doc is the whole context\n",
    "    # return: a list of sentences.\n",
    "    lines = doc.strip().split('\\n')\n",
    "    sentences = [line.split('\\t') for line in lines]\n",
    "    return sentences\n",
    "\n",
    "def clean_data(lines):\n",
    "    # @para: lines is a list of lists, aka a list of all sentences\n",
    "    # return sentences with only words, seperated by spaceÃŸ. \n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha() and word != 'unk']\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_token(clean_lines):\n",
    "    # @paras: input is clean_lines\n",
    "    # returns: vocab-a dictionary matches word to index. word as key, index as value\n",
    "    # sequences: list of sentences, represented by number\n",
    "    # mmax: length of the Slongest sentence \n",
    "    \n",
    "    # create a word_set contains all words\n",
    "    word_set = set()\n",
    "    for line in clean_lines:\n",
    "        cur_set = set(line[0].split())\n",
    "        word_set = word_set.union(cur_set)\n",
    "    word_list = sorted(word_set) # a sorted list contains all words\n",
    "    vocab = dict() # word as key, index as value\n",
    "    for i in range(len(word_list)):\n",
    "        vocab[word_list[i]] = i+1\n",
    "    # text into tokens. \n",
    "    mmax = 0\n",
    "    sequences = []\n",
    "    for line in clean_lines:\n",
    "        words = line[0].split()\n",
    "        tokens = []\n",
    "        for word in words:\n",
    "            idx = vocab[word]\n",
    "            tokens.append(idx)\n",
    "        sequences.append(tokens)\n",
    "        mmax = len(words) if len(words) > mmax else mmax\n",
    "    return vocab, sequences, mmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = load_doc('a3-data/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = to_sentence(f1) # convert how passage to sentences\n",
    "clean_lines = clean_data(lines) # 2d list, each sentence for one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2d list to string list. each sentence as a string\n",
    "# because the input of tokenizer should be a string list\n",
    "corpus = []\n",
    "for c in clean_lines:\n",
    "    corpus.append(c[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus) # feed tokenizer with only TRAINING set!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = len(tokenizer.word_index) + 1 # +1 for zero padding, word_index starts from 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9885"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[: i + 1]  \n",
    "        input_sequences.append(n_gram_sequence)\n",
    "max_sequence_length = max([len(x) for x in input_sequences])\n",
    "padded_seq = np.array(pad_sequences(input_sequences, maxlen=20, padding='pre')) # window 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "790431"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "import pickle\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# input of training\n",
    "predictors, labels = padded_seq[:, :-1], padded_seq[:, -1] # use 19 to train, predict the 20th word\n",
    "labels = ku.to_categorical(labels, num_classes=total_words) # onehot encoding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,    0, 9855],\n",
       "       [   0,    0,    0, ...,    0, 9855, 9856],\n",
       "       [   0,    0,    0, ..., 9855, 9856, 9857],\n",
       "       ...,\n",
       "       [ 635,  743,   10, ...,   56,  241, 4076],\n",
       "       [ 743,   10,  465, ...,  241, 4076,  210],\n",
       "       [  10,  465,  871, ..., 4076,  210,    4]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "500/500 [==============================] - 10s 20ms/step - loss: 9.1919\n",
      "\n",
      "Epoch 00001: loss improved from inf to 9.19194, saving model to saved-best-model.hdf5\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 8.4096\n",
      "\n",
      "Epoch 00002: loss improved from 9.19194 to 8.40960, saving model to saved-best-model.hdf5\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.9270\n",
      "\n",
      "Epoch 00003: loss improved from 8.40960 to 5.92698, saving model to saved-best-model.hdf5\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.4411\n",
      "\n",
      "Epoch 00004: loss improved from 5.92698 to 5.44112, saving model to saved-best-model.hdf5\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.2827\n",
      "\n",
      "Epoch 00005: loss improved from 5.44112 to 5.28271, saving model to saved-best-model.hdf5\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 5.2210\n",
      "\n",
      "Epoch 00006: loss improved from 5.28271 to 5.22102, saving model to saved-best-model.hdf5\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.2087\n",
      "\n",
      "Epoch 00007: loss improved from 5.22102 to 5.20869, saving model to saved-best-model.hdf5\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.1869\n",
      "\n",
      "Epoch 00008: loss improved from 5.20869 to 5.18689, saving model to saved-best-model.hdf5\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.1850\n",
      "\n",
      "Epoch 00009: loss improved from 5.18689 to 5.18502, saving model to saved-best-model.hdf5\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.1698\n",
      "\n",
      "Epoch 00010: loss improved from 5.18502 to 5.16979, saving model to saved-best-model.hdf5\n"
     ]
    }
   ],
   "source": [
    "filepath = \"saved-best-model.hdf5\"\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 15, input_length=19)) # input_length: predictor size\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(total_words, activation='softmax')) # outputlayer with vocab size\n",
    "adam = keras.optimizers.Adam(lr=0.001) # set learning rate to 0.001(which is default)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# create checkpoint to save weights for each epoch\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='auto', period=1)\n",
    "model.fit(predictors[:500], labels[:500], epochs=10, batch_size=50, verbose=1, callbacks=[checkpoint])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 5.1794\n",
      "\n",
      "Epoch 00001: loss did not improve from 5.16979\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.1752\n",
      "\n",
      "Epoch 00002: loss did not improve from 5.16979\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.1693\n",
      "\n",
      "Epoch 00003: loss improved from 5.16979 to 5.16926, saving model to saved-best-model.hdf5\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.1922\n",
      "\n",
      "Epoch 00004: loss did not improve from 5.16926\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.1814\n",
      "\n",
      "Epoch 00005: loss did not improve from 5.16926\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 5.1497\n",
      "\n",
      "Epoch 00006: loss improved from 5.16926 to 5.14974, saving model to saved-best-model.hdf5\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.1779\n",
      "\n",
      "Epoch 00007: loss did not improve from 5.14974\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.1879\n",
      "\n",
      "Epoch 00008: loss did not improve from 5.14974\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.1727\n",
      "\n",
      "Epoch 00009: loss did not improve from 5.14974\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.1557\n",
      "\n",
      "Epoch 00010: loss did not improve from 5.14974\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23f10292e8>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(\"saved-best-model.hdf5\")\n",
    "model.fit(predictors[:500], labels[:500], epochs=10, batch_size=50, verbose=1, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 9.1920\n",
      "\n",
      "Epoch 00001: saving model to saved-model-01-9.19.hdf5\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 8.3492\n",
      "\n",
      "Epoch 00002: saving model to saved-model-02-8.35.hdf5\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.9335\n",
      "\n",
      "Epoch 00003: saving model to saved-model-03-5.93.hdf5\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.4096\n",
      "\n",
      "Epoch 00004: saving model to saved-model-04-5.41.hdf5\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.2890\n",
      "\n",
      "Epoch 00005: saving model to saved-model-05-5.29.hdf5\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.2001\n",
      "\n",
      "Epoch 00006: saving model to saved-model-06-5.20.hdf5\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.2038\n",
      "\n",
      "Epoch 00007: saving model to saved-model-07-5.20.hdf5\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.1893\n",
      "\n",
      "Epoch 00008: saving model to saved-model-08-5.19.hdf5\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.1833\n",
      "\n",
      "Epoch 00009: saving model to saved-model-09-5.18.hdf5\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.1871\n",
      "\n",
      "Epoch 00010: saving model to saved-model-10-5.19.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23e7f4e3c8>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = \"saved-model-{epoch:02d}-{loss:.2f}.hdf5\"\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 15, input_length=19)) # input_length: predictor size\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(total_words, activation='softmax')) # outputlayer with vocab size\n",
    "adam = keras.optimizers.Adam(lr=0.001) # set learning rate to 0.001(which is default)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=False, mode='auto', period=1)\n",
    "model.fit(predictors[:500], labels[:500], epochs=10, batch_size=50, verbose=1, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./final_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "tensorboard = TensorBoard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks.append(tensorboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 9.1924\n",
      "\n",
      "Epoch 00001: saving model to saved-model-01-9.19.hdf5\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 8.3929\n",
      "\n",
      "Epoch 00002: saving model to saved-model-02-8.39.hdf5\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.9856\n",
      "\n",
      "Epoch 00003: saving model to saved-model-03-5.99.hdf5\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.4481\n",
      "\n",
      "Epoch 00004: saving model to saved-model-04-5.45.hdf5\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.2933\n",
      "\n",
      "Epoch 00005: saving model to saved-model-05-5.29.hdf5\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.1975\n",
      "\n",
      "Epoch 00006: saving model to saved-model-06-5.20.hdf5\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.2031\n",
      "\n",
      "Epoch 00007: saving model to saved-model-07-5.20.hdf5\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.1872\n",
      "\n",
      "Epoch 00008: saving model to saved-model-08-5.19.hdf5\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.1768\n",
      "\n",
      "Epoch 00009: saving model to saved-model-09-5.18.hdf5\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 2s 5ms/step - loss: 5.1889\n",
      "\n",
      "Epoch 00010: saving model to saved-model-10-5.19.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23e904bb38>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_string=model.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2007"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open('model_test.json', 'w').write(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('test_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_seq[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_length, vocab_size):\n",
    "\n",
    "    input_len = max_length - 1\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add embedding layer\n",
    "    # max_length is the biggest sequence length\n",
    "    # 10 is the size for each word vector after embedding layer\n",
    "    # max_length is N in the ngram \n",
    "    model.add(Embedding(vocab_size, 10, input_length=input_len))\n",
    "    \n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    # the last layer implement softmax on the whole vocabulary\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7110"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 69, 10)            71100     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 7110)              718110    \n",
      "=================================================================\n",
      "Total params: 833,610\n",
      "Trainable params: 833,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(max_length, vocab_size)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,  138,  554,  656,  900, 1005, 1164, 2645, 2740, 2846,\n",
       "       3063, 3332, 3453, 3909, 4021, 4115, 5024, 5077, 5216, 5494, 5809,\n",
       "       5884, 6013, 6277], dtype=int32)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Epoch 2/100\n",
      "Epoch 3/100\n",
      "Epoch 4/100\n",
      "Epoch 5/100\n",
      "Epoch 6/100\n",
      "Epoch 7/100\n",
      "Epoch 8/100\n",
      "Epoch 9/100\n",
      "Epoch 10/100\n",
      "Epoch 11/100\n",
      "Epoch 12/100\n",
      "Epoch 13/100\n",
      "Epoch 14/100\n",
      "Epoch 15/100\n",
      "Epoch 16/100\n",
      "Epoch 17/100\n",
      "Epoch 18/100\n",
      "Epoch 19/100\n",
      "Epoch 20/100\n",
      "Epoch 21/100\n",
      "Epoch 22/100\n",
      "Epoch 23/100\n",
      "Epoch 24/100\n",
      "Epoch 25/100\n",
      "Epoch 26/100\n",
      "Epoch 27/100\n",
      "Epoch 28/100\n",
      "Epoch 29/100\n",
      "Epoch 30/100\n",
      "Epoch 31/100\n",
      "Epoch 32/100\n",
      "Epoch 33/100\n",
      "Epoch 34/100\n",
      "Epoch 35/100\n",
      "Epoch 36/100\n",
      "Epoch 37/100\n",
      "Epoch 38/100\n",
      "Epoch 39/100\n",
      "Epoch 40/100\n",
      "Epoch 41/100\n",
      "Epoch 42/100\n",
      "Epoch 43/100\n",
      "Epoch 44/100\n",
      "Epoch 45/100\n",
      "Epoch 46/100\n",
      "Epoch 47/100\n",
      "Epoch 48/100\n",
      "Epoch 49/100\n",
      "Epoch 50/100\n",
      "Epoch 51/100\n",
      "Epoch 52/100\n",
      "Epoch 53/100\n",
      "Epoch 54/100\n",
      "Epoch 55/100\n",
      "Epoch 56/100\n",
      "Epoch 57/100\n",
      "Epoch 58/100\n",
      "Epoch 59/100\n",
      "Epoch 60/100\n",
      "Epoch 61/100\n",
      "Epoch 62/100\n",
      "Epoch 63/100\n",
      "Epoch 64/100\n",
      "Epoch 65/100\n",
      "Epoch 66/100\n",
      "Epoch 67/100\n",
      "Epoch 68/100\n",
      "Epoch 69/100\n",
      "Epoch 70/100\n",
      "Epoch 71/100\n",
      "Epoch 72/100\n",
      "Epoch 73/100\n",
      "Epoch 74/100\n",
      "Epoch 75/100\n",
      "Epoch 76/100\n",
      "Epoch 77/100\n",
      "Epoch 78/100\n",
      "Epoch 79/100\n",
      "Epoch 80/100\n",
      "Epoch 81/100\n",
      "Epoch 82/100\n",
      "Epoch 83/100\n",
      "Epoch 84/100\n",
      "Epoch 85/100\n",
      "Epoch 86/100\n",
      "Epoch 87/100\n",
      "Epoch 88/100\n",
      "Epoch 89/100\n",
      "Epoch 90/100\n",
      "Epoch 91/100\n",
      "Epoch 92/100\n",
      "Epoch 93/100\n",
      "Epoch 94/100\n",
      "Epoch 95/100\n",
      "Epoch 96/100\n",
      "Epoch 97/100\n",
      "Epoch 98/100\n",
      "Epoch 99/100\n",
      "Epoch 100/100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x6598c4f90>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(predictors, labels, epochs=100, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(input_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f2 = load_doc('a3-data/valid.txt')\n",
    "f2_lines = to_sentence(f2)\n",
    "f2_clean_lines = clean_data(f2_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,  138,  554,  656,  900, 1005, 1164, 2645, 2740, 2846,\n",
       "       3063, 3332, 3453, 3909, 4021, 4115, 5024, 5077, 5216, 5494, 5809,\n",
       "       5884, 6013, 6277], dtype=int32)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected embedding_4_input to have shape (69,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-305c6173e89d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mval_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda3/envs/nlp/lib/python3.7/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36mpredict_classes\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \"\"\"\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/nlp/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/nlp/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/nlp/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected embedding_4_input to have shape (69,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "val_pred = model.predict_classes(val_in, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2_tokens=[]\n",
    "\n",
    "for line in f2_clean_lines:\n",
    "    # ues the training vocabulary, match text in validation dataset to numbers\n",
    "    # if the word doesn't exist in the dictionary, it's zero.\n",
    "    # the length of each input has to match the model, in this case max_length\n",
    "    words = line[0].split()\n",
    "    token = np.zeros(len(words))\n",
    "    for i in range(len(words)):\n",
    "        if words[i] in vocab:\n",
    "            token[i] = vocab[words[i]]\n",
    "    f2_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = to_sentence(f1)\n",
    "clean_lines = clean_data(lines)\n",
    "vocab, sequences, max_length = to_token(clean_lines)\n",
    "vocab_size = len(vocab) + 1\n",
    "# max_length = 10\n",
    "input_seq = np.array(pad_sequences(sequences, maxlen=max_length, padding='pre'))\n",
    "    \n",
    "predictors, labels = input_seq[:,:-1],input_seq[:,-1]\n",
    "labels = ku.to_categorical(labels, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(text):  # text is a string list, for one paragraph \n",
    "#     # remove line breaks \n",
    "#     text = text.strip()\n",
    "#     text = text.replace('\\r', ' ').replace('\\n', ' ')\n",
    "#     text = ' '.join(text.split())\n",
    "    \n",
    "#     # remove punctuations from paragraphs\n",
    "#     text = ''.join(c for c in text if c not in punctuation)\n",
    "#     # change 's \n",
    "#     text = re.sub('\\t', ' ', text)\n",
    "#     text = re.sub(r\"it\\'s\",\"it is\",text)\n",
    "#     text = re.sub(r\"i\\'d\",\"i would\",text)\n",
    "#     text = re.sub(r\"I\\'d\",\"I would\",text)\n",
    "#     text = re.sub(r\"don\\'t\",\"do not\",text)\n",
    "#     text = re.sub(r\"he\\'s\",\"he is\",text)\n",
    "#     text = re.sub(r\"there\\'s\",\"there is\",text)\n",
    "#     text = re.sub(r\"that\\'s\",\"that is\",text)\n",
    "#     text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "#     text = re.sub(r\"cannot\", \"can not \", text)\n",
    "#     text = re.sub(r\"what\\'s\", \"what is\", text)\n",
    "#     text = re.sub(r\"What\\'s\", \"what is\", text)\n",
    "#     text = re.sub(r\"\\'ve \", \" have \", text)\n",
    "#     text = re.sub(r\"n\\'t\", \" not \", text)\n",
    "#     text = re.sub(r\"i\\'m\", \"i am \", text)\n",
    "#     text = re.sub(r\"I\\'m\", \"I am \", text)\n",
    "#     text = re.sub(r\"\\'re\", \" are \", text)\n",
    "#     text = re.sub(r\"\\'d\", \" would \", text)\n",
    "#     text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "#     text = re.sub(r\"\\'s\",\" is\",text)\n",
    "    \n",
    "#     text = re.sub('[^a-zA-Z]', ' ', text)  # only keep characters\n",
    "#     p = text.lower()  # to lowercase\n",
    "    \n",
    "#     # remove line breaks \n",
    "#     text = text.strip()\n",
    "#     text = text.replace('\\r', ' ').replace('\\n', ' ')\n",
    "#     text = ' '.join(text.split())\n",
    "#     text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "#     text = text.lower()\n",
    "#     tokens = word_tokenize(text)  \n",
    "#     tokens = [w for w in tokens if w != 'unk']\n",
    "#     return ' '.join(tokens) # return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_5k = preprocess(txt_5k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(train_5k.split())\n",
    "words = sorted(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7113"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.array(words) == words[0])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7113,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehotvec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_array = np.random.randint(1000, size=(32, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(1000, 64, input_length=10))\n",
    "# the model will take as input an integer matrix of size (batch, input_length).\n",
    "# the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size).\n",
    "# now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
    "\n",
    "input_array = np.random.randint(1000, size=(32, 10))\n",
    "\n",
    "model.compile('rmsprop', 'mse')\n",
    "output_array = model.predict(input_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_idx['calloway']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'calloway'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_5k[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_matrix = []\n",
    "word_idx = {} # a dictionary to match word with volcabulary index\n",
    "# one hot encoding\n",
    "for i in range(len(words)):\n",
    "    word_idx[words[i]] = i\n",
    "    onehotvec = np.zeros(len(words))\n",
    "    onehotvec[i] = 1\n",
    "    word_matrix.append(onehotvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# represent word by index of volcabulary\n",
    "train = []\n",
    "for i in train_5k:\n",
    "    cur_idx = word_idx[i]\n",
    "    train.append(cur_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# word embedding, volcabulary size: 7113, output word vector dim:64\n",
    "# 7114: volab size+1, 64: output vector size, 50: batch size for training\n",
    "model.complile('rmsprop', 'mse')\n",
    "model.add(Embedding(7114, 64, input_length=50)) \n",
    "output_array = model.predict(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
