{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re\n",
    "import string\n",
    "from unicodedata import normalize\n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply, Reshape\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda, Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model, Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.backend as K\n",
    "import keras\n",
    "import numpy as np\n",
    "import random\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    return pairs\n",
    "\n",
    "def clean_data(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return np.array(cleaned)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2sequences(max_len, lines):\n",
    "    tokenizer = Tokenizer(oov_token='<unk>')\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    seqs = tokenizer.texts_to_sequences(lines)\n",
    "    seqs_pad = pad_sequences(seqs, maxlen=max_len, padding='post')\n",
    "    return seqs_pad, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customize softmax function\n",
    "def softmax(x, axis=1):\n",
    "    \"\"\"\n",
    "    Softmax activation function.\n",
    "    \"\"\"\n",
    "    ndim = K.ndim(x)\n",
    "    if ndim == 2:\n",
    "        return K.softmax(x)\n",
    "    elif ndim > 2:\n",
    "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "        s = K.sum(e, axis=axis, keepdims=True)\n",
    "        return e / s\n",
    "    else:\n",
    "        raise ValueError('Cannot apply softmax to a tensor that is 1D')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Attention mechanism, return weighted Context Vector\n",
    "    \n",
    "    @param a: hidden state of BiRNN\n",
    "    @param s_prev: last state of Decoder LSTM\n",
    "    \n",
    "    Returns:\n",
    "    context: weighted Context Vector\n",
    "    \"\"\"\n",
    "    \n",
    "    # repeat s_prev Tx times, one for each word\n",
    "    s_prev = repeator(s_prev)\n",
    "    # connect BiRNN hidden state to s_prev\n",
    "    concat = concatenator([a, s_prev])\n",
    "    # compute energies\n",
    "    e = densor_tanh(concat)\n",
    "    energies = densor_relu(e)\n",
    "    # compute weights\n",
    "    alphas = activator(energies)\n",
    "    # get weighted Context Vector\n",
    "    context = dotor([alphas, a])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, source_vocab_to_int):\n",
    "    \"\"\"\n",
    "    build Embedding layer and pretrain word embedding\n",
    "\n",
    "    @param word_to_vec_map: word to vector\n",
    "    @param word_to_index: word to one hot encoding\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(source_vocab_to_int) + 1        # Keras Embedding API +1\n",
    "    emb_dim = word_to_vec_map[\"the\"].shape[0]\n",
    "    \n",
    "    # initialize embedding matrix\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    # fit wordvec to embedding layers\n",
    "    for word, index in source_vocab_to_int.items():\n",
    "        word_vector = word_to_vec_map.get(word, np.zeros(emb_dim))\n",
    "        emb_matrix[index, :] = word_vector\n",
    "\n",
    "    # build embedding layer, cannot be trained\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "\n",
    "    # build embedding layer\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # set weights\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"fra.txt\"\n",
    "doc = load_doc(filename)\n",
    "pairs = to_pairs(doc)\n",
    "\n",
    "# choose sample size\n",
    "# considering the time consuming, only take 30000 samples as training and testing data set \n",
    "n_train = 30000\n",
    "clean_pairs = clean_data(pairs)[0:n_train, :]\n",
    "# clean_pairs = clean_data(pairs)\n",
    "input_texts = clean_pairs[:, 0]\n",
    "target_texts = clean_pairs[:, 1]\n",
    "\n",
    "# create word level input sequence\n",
    "input_sequences = []\n",
    "for t in input_texts:\n",
    "    input_sequences.append(t.split())\n",
    "# create word level target sequence\n",
    "target_sequences = []\n",
    "for t in target_texts:\n",
    "    cur_seq = t.split()\n",
    "    cur_seq.append('<eos>') # add end sentence lable\n",
    "    target_sequences.append(cur_seq)\n",
    "\n",
    "max_encoder_seq_length = max(len(line) for line in input_sequences)\n",
    "max_decoder_seq_length = max(len(line) for line in target_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(input_sequences, target_sequences, test_size=0.2, random_state=42)\n",
    "source_text_to_int, source_vocab_to_int = text2sequences(max_encoder_seq_length, X_train)\n",
    "target_text_to_int, target_vocab_to_int = text2sequences(max_decoder_seq_length, y_train)\n",
    "\n",
    "source_vocab_to_int['<pad>'] = 0\n",
    "target_vocab_to_int['<pad>'] = 0\n",
    "\n",
    "source_int_to_vocab = {word: idx for idx, word in source_vocab_to_int.items()}\n",
    "target_int_to_vocab = {word: idx for idx, word in target_vocab_to_int.items()}\n",
    "\n",
    "X = source_text_to_int\n",
    "Y = target_text_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 7)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_text_to_int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# onehot encoding\n",
    "Xoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(source_vocab_to_int)), X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(target_vocab_to_int)), Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx = max_encoder_seq_length \n",
    "Ty = max_decoder_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/joey/Desktop/C/CS584/assignment5/venv/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# load pretrained word embedding from glove\n",
    "with open(\"glove.6B/glove.6B.50d.txt\", 'r') as f:\n",
    "    words = set()\n",
    "    word_to_vec_map = {}\n",
    "    for line in f:\n",
    "        line = line.strip().split()\n",
    "        curr_word = line[0]\n",
    "        words.add(curr_word)\n",
    "        word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "\n",
    "# Embedding layer\n",
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, source_vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor_tanh = Dense(32, activation = \"tanh\")\n",
    "densor_relu = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights')\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 32 # The hidden size of Bi-LSTM\n",
    "n_s = 128 # The hidden size of LSTM in Decoder\n",
    "decoder_LSTM_cell = LSTM(n_s, return_state=True)\n",
    "output_layer = Dense(len(target_vocab_to_int), activation=softmax)\n",
    "\n",
    "# define model layers\n",
    "reshapor = Reshape((1, len(target_vocab_to_int)))\n",
    "concator = Concatenate(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(Tx, Ty, n_a, n_s, source_vocab_size, target_vocab_size):\n",
    "    # input layer\n",
    "    X = Input(shape=(Tx,))\n",
    "    # Embedding layer\n",
    "    embed = embedding_layer(X)\n",
    "    # initialize Decoder LSTM\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    \n",
    "     # Decoder input for LSTM layer\n",
    "    out0 = Input(shape=(target_vocab_size, ), name='out0')\n",
    "    out = reshapor(out0)\n",
    "    \n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    # save outputs\n",
    "    outputs = []\n",
    "    \n",
    "    # define Bi-LSTM\n",
    "    a = Bidirectional(LSTM(n_a, return_sequences=True))(embed)\n",
    "    \n",
    "    # Decoder, iterate max_decoder_seq_length rounds, each iteration generates one result\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # get Context Vector\n",
    "        context = one_step_attention(a, s)\n",
    "        \n",
    "        # concat Context Vector and the previous translated result\n",
    "        context = concator([context, reshapor(out)])\n",
    "        s, _, c = decoder_LSTM_cell(context, initial_state=[s, c])\n",
    "        \n",
    "        # connect lstm output and dense layer\n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # save output result\n",
    "        outputs.append(out)\n",
    "    \n",
    "    model = Model([X, s0, c0, out0], outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(sentence, Tx):\n",
    "    \"\"\"\n",
    "    make predictions on given sentences\n",
    "    \"\"\"\n",
    "    # encoding \n",
    "    unk_idx = source_vocab_to_int[\"<unk>\"]\n",
    "    word_idx = [source_vocab_to_int.get(word, unk_idx) for word in sentence]\n",
    "    word_idx = np.array(word_idx + [0] * (Tx - len(word_idx)))\n",
    "    \n",
    "    # translated results\n",
    "    preds = model.predict([word_idx.reshape(-1,Tx), s0, c0, out0])\n",
    "    predictions = np.argmax(preds, axis=-1)\n",
    "    \n",
    "    # to words\n",
    "    pred_words = [target_int_to_vocab.get(idx[0], \"<unk>\") for idx in predictions]\n",
    "    pred_string = \" \".join(pred_words)\n",
    "    pred_french = pred_string.split('<eos>')[0]\n",
    "    return pred_french.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.logs = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.logs.append(logs['loss'])\n",
    "        if self.i % 200 == 0:\n",
    "            print('Loss for {} iteration:'.format(self.i), logs['loss'])\n",
    "        self.i += 1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/joey/Desktop/C/CS584/assignment5/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1238: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Users/joey/Desktop/C/CS584/assignment5/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1204: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "model = build_model(Tx, Ty, n_a, n_s, len(source_vocab_to_int), len(target_vocab_to_int))\n",
    "his = LossHistory()\n",
    "out = model.compile(optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0.001),\n",
    "                    metrics=['accuracy'],\n",
    "                    loss='categorical_crossentropy')\n",
    "\n",
    "\n",
    "m = X.shape[0] # num of training sample\n",
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "out0 = np.zeros((m, len(target_vocab_to_int)))\n",
    "outputs = list(Yoh.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"final_seq2seq_model_1121.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for 0 iteration: 15.428822\n",
      "Loss for 200 iteration: 15.181914\n",
      "Loss for 400 iteration: 16.751379\n",
      "Loss for 600 iteration: 15.314741\n",
      "Loss for 800 iteration: 14.052397\n",
      "Loss for 1000 iteration: 14.170492\n",
      "Loss for 1200 iteration: 15.35292\n",
      "Loss for 1400 iteration: 13.94512\n",
      "Loss for 1600 iteration: 15.068814\n",
      "Loss for 1800 iteration: 13.895139\n",
      "Loss for 2000 iteration: 15.116055\n",
      "Loss for 2200 iteration: 14.029553\n",
      "Loss for 2400 iteration: 15.572563\n",
      "Loss for 2600 iteration: 14.693933\n",
      "Loss for 2800 iteration: 14.49578\n",
      "Loss for 3000 iteration: 14.497648\n",
      "Loss for 3200 iteration: 15.241299\n",
      "Loss for 3400 iteration: 13.200629\n",
      "Loss for 3600 iteration: 15.295173\n"
     ]
    }
   ],
   "source": [
    "model.fit([X, s0, c0, out0], outputs, \n",
    "          epochs=20, \n",
    "          batch_size=128,\n",
    "          verbose=0,\n",
    "          callbacks=[his]\n",
    "         )\n",
    "# save weights\n",
    "model.save_weights(\"final_seq2seq_model_1121.h5\") # 100 epoch in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = random.sample(range(len(X_test)), 20) # randomly choose 20 sample from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je veux le <eos> <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "sentence = ['i', 'want', 'this', 'cat']\n",
    "unk_idx = source_vocab_to_int[\"<unk>\"]\n",
    "word_idx = [source_vocab_to_int.get(word, unk_idx) for word in sentence]\n",
    "word_idx = np.array(word_idx + [0] * (Tx - len(word_idx)))\n",
    "# translated results\n",
    "preds = model.predict([word_idx.reshape(-1,Tx), s0, c0, out0])\n",
    "predictions = np.argmax(preds, axis=-1) \n",
    "# to words\n",
    "pred_words = [target_int_to_vocab.get(idx[0], \"<unk>\") for idx in predictions]\n",
    "pred_string = \" \".join(pred_words)\n",
    "print(pred_string)\n",
    "#     pred_french = pred_string.split('<eos>')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "English:\t ['tom', 'has', 'a', 'dog']\n",
      "French(true):\t ['tom', 'a', 'un', 'chien']\n",
      "French(pred):\t ['tom', 'est', 'un', 'un']\n",
      "-\n",
      "English:\t ['stay', 'down']\n",
      "French(true):\t ['reste', 'baisse']\n",
      "French(pred):\t ['reste']\n",
      "-\n",
      "English:\t ['youre', 'fun']\n",
      "French(true):\t ['tes', 'marrante']\n",
      "French(pred):\t ['vous', 'etes']\n",
      "-\n",
      "English:\t ['do', 'you', 'believe', 'him']\n",
      "French(true):\t ['estce', 'que', 'tu', 'le', 'crois', 'lui']\n",
      "French(pred):\t ['tu', 'que']\n",
      "-\n",
      "English:\t ['here', 'look', 'at', 'this']\n",
      "French(true):\t ['voila', 'regarde', 'ca']\n",
      "French(pred):\t ['regardez', 'la']\n",
      "-\n",
      "English:\t ['he', 'doesnt', 'want', 'it']\n",
      "French(true):\t ['il', 'nen', 'veut', 'pas']\n",
      "French(pred):\t ['il', 'ne', 'pas', 'pas']\n",
      "-\n",
      "English:\t ['itll', 'break']\n",
      "French(true):\t ['ca', 'va', 'rompre']\n",
      "French(pred):\t ['tout', 'est', 'monde']\n",
      "-\n",
      "English:\t ['its', 'the', 'best']\n",
      "French(true):\t ['cest', 'le', 'meilleur']\n",
      "French(pred):\t ['cest', 'un']\n",
      "-\n",
      "English:\t ['lets', 'take', 'a', 'walk']\n",
      "French(true):\t ['allons', 'marcher']\n",
      "French(pred):\t ['allons', 'un']\n",
      "-\n",
      "English:\t ['they', 'let', 'me', 'go']\n",
      "French(true):\t ['elles', 'mont', 'laisse', 'partir']\n",
      "French(pred):\t ['elles', 'mont', 'laisserent']\n",
      "-\n",
      "English:\t ['im', 'not', 'at', 'all', 'busy']\n",
      "French(true):\t ['je', 'ne', 'suis', 'pas', 'occupe', 'du', 'tout']\n",
      "French(pred):\t ['je', 'ne', 'suis', 'pas', 'de', 'de']\n",
      "-\n",
      "English:\t ['i', 'have', 'an', 'old', 'car']\n",
      "French(true):\t ['jai', 'une', 'vieille', 'voiture']\n",
      "French(pred):\t ['jai', 'un', 'un']\n",
      "-\n",
      "English:\t ['you', 'were', 'fantastic']\n",
      "French(true):\t ['vous', 'avez', 'ete', 'fantastique']\n",
      "French(pred):\t ['tu', 'es', 'tres']\n",
      "-\n",
      "English:\t ['she', 'is', 'active']\n",
      "French(true):\t ['elle', 'est', 'active']\n",
      "French(pred):\t ['elle', 'est', 'obstinee']\n",
      "-\n",
      "English:\t ['you', 'look', 'very', 'good']\n",
      "French(true):\t ['vous', 'avez', 'lair', 'tres', 'bien']\n",
      "French(pred):\t ['tu', 'as', 'lair', 'tres']\n",
      "-\n",
      "English:\t ['youre', 'fortunate']\n",
      "French(true):\t ['tu', 'as', 'de', 'la', 'chance']\n",
      "French(pred):\t ['vous', 'etes']\n",
      "-\n",
      "English:\t ['youre', 'not', 'normal']\n",
      "French(true):\t ['tu', 'nes', 'pas', 'normale']\n",
      "French(pred):\t ['vous', 'netes', 'pas', 'mechante']\n",
      "-\n",
      "English:\t ['it', 'could', 'be', 'anybody']\n",
      "French(true):\t ['ca', 'pourrait', 'etre', 'nimporte', 'qui']\n",
      "French(pred):\t ['ca', 'ne', 'etre']\n",
      "-\n",
      "English:\t ['say', 'hello']\n",
      "French(true):\t ['dis', 'bonjour']\n",
      "French(pred):\t ['reste']\n",
      "-\n",
      "English:\t ['she', 'liked', 'that']\n",
      "French(true):\t ['elle', 'la', 'apprecie']\n",
      "French(pred):\t ['elle', 'est']\n"
     ]
    }
   ],
   "source": [
    "for idx in test_list:\n",
    "    candidate = make_prediction(X_test[idx], Tx)\n",
    "    reference = y_test[idx][:-1]\n",
    "    print('-')\n",
    "    print('English:\t', X_test[idx])\n",
    "    print('French(true):\t', reference)\n",
    "    print('French(pred):\t', candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE IS : 0.29483933604682255\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "# evaluate model by test dataset\n",
    "# print bleu score \n",
    "trueFrench = []\n",
    "predFrench = []\n",
    "sum_score = 0\n",
    "for i in range(len(X_test)):\n",
    "    candidate = make_prediction(X_test[i], Tx)\n",
    "    reference = y_test[i][:-1]\n",
    "    predFrench.append(candidate)\n",
    "    trueFrench.append(reference)\n",
    "    score = sentence_bleu([reference], candidate, weights=(1,0,0,0))\n",
    "    sum_score += score\n",
    "avg = sum_score / len(X_test)\n",
    "print('SCORE IS :', avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
