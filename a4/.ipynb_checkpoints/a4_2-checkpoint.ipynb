{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 261, 100)          1474300   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 257, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 86, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 82, 64)            41024     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5248)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5248)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 5248)              20992     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                335936    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,936,445\n",
      "Trainable params: 1,925,949\n",
      "Non-trainable params: 10,496\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from string import punctuation\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras import regularizers, layers\n",
    "from keras.layers import Embedding, Dense, Dropout, Flatten, BatchNormalization, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "f = open('datasetSplit.txt')\n",
    "text = f.read()\n",
    "f.close()\n",
    "text = text.split()\n",
    "idx_label = {} # key is train/val/test, value is the sentence number\n",
    "idx_label['1'] = []\n",
    "idx_label['2'] = []\n",
    "idx_label['3'] = []\n",
    "for i in text[1:]:\n",
    "    try:\n",
    "        x = i.split(',')\n",
    "        label = x[1]\n",
    "        idx = x[0]\n",
    "        idx_label[label].append(idx)\n",
    "    except ValueError:\n",
    "        print(i)\n",
    "\n",
    "\n",
    "f = open('datasetSentences.txt')\n",
    "text_sentences = f.read()\n",
    "f.close()\n",
    "text_sentences = text_sentences.split('\\n')\n",
    "idx_sentence = {} # key is the number of sentence, value is the sentence text\n",
    "for s in text_sentences[1:]:\n",
    "    try:\n",
    "        ss = s.split('\\t')\n",
    "        idx = ss[0]\n",
    "        sentence = ss[1]\n",
    "        idx_sentence[idx] = sentence\n",
    "    except IndexError:\n",
    "        print(s)\n",
    "\n",
    "\n",
    "f = open('sentiment_labels.txt')\n",
    "text = f.readlines()\n",
    "f.close()\n",
    "idx_sentiment = {} # key is the number of sentence, value is the sentiment\n",
    "for s in text[1:]:\n",
    "    try:\n",
    "        ss = s.split('|')\n",
    "        idx = ss[0]\n",
    "        sentiment = ss[1].split('\\n')\n",
    "        idx_sentiment[idx] = float(sentiment[0])\n",
    "    except IndexError:\n",
    "        print(s)\n",
    "\n",
    "\n",
    "f = open('dictionary.txt')\n",
    "text = f.read()\n",
    "f.close()\n",
    "lines = text.split('\\n')\n",
    "phrase_sentiment = {}\n",
    "for line in lines:\n",
    "    try:\n",
    "        ss = line.split('|')\n",
    "        phrase = ss[0]\n",
    "        idx = ss[1]\n",
    "        phrase_sentiment[phrase] = idx\n",
    "    except IndexError:\n",
    "        print(line)\n",
    "\n",
    "\n",
    "Xtrain = []\n",
    "Xval = []\n",
    "Xtest = []\n",
    "for idx in idx_label['1']: # training set\n",
    "    Xtrain.append(idx_sentence[idx])\n",
    "for idx in idx_label['2']: # training set\n",
    "    Xval.append(idx_sentence[idx])\n",
    "for idx in idx_label['3']: # training set\n",
    "    Xtest.append(idx_sentence[idx])\n",
    "\n",
    "\n",
    "ytrain = {}\n",
    "yval = {}\n",
    "ytest = {}\n",
    "notin = []\n",
    "for i in Xtrain:\n",
    "    if i not in phrase_sentiment:\n",
    "        notin.append(i)\n",
    "        continue\n",
    "    senti_idx = phrase_sentiment[i]\n",
    "    sentiment = idx_sentiment[senti_idx]\n",
    "    ytrain[i] = sentiment\n",
    "    \n",
    "for i in Xval:\n",
    "    if i not in phrase_sentiment:\n",
    "        notin.append(i)\n",
    "        continue\n",
    "    senti_idx = phrase_sentiment[i]\n",
    "    sentiment = idx_sentiment[senti_idx]\n",
    "    yval[i] = sentiment\n",
    "\n",
    "for i in Xtest:\n",
    "    if i not in phrase_sentiment:\n",
    "        notin.append(i)\n",
    "        continue\n",
    "    senti_idx = phrase_sentiment[i]\n",
    "    sentiment = idx_sentiment[senti_idx]\n",
    "    ytest[i] = sentiment\n",
    "\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "for k in ytrain:\n",
    "    y = ytrain[k]\n",
    "    x_train.append(k)\n",
    "    y_train.append(y)\n",
    "\n",
    "x_val = []\n",
    "y_val = []\n",
    "for k in yval:\n",
    "    y = yval[k]\n",
    "    x_val.append(k)\n",
    "    y_val.append(y)\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "for k in ytest:\n",
    "    y = ytest[k]\n",
    "    x_test.append(k)\n",
    "    y_test.append(y)\n",
    "\n",
    "max_sentence_len = 0\n",
    "for x in x_train:\n",
    "    length = len(x)\n",
    "    max_sentence_len = length if length > max_sentence_len else max_sentence_len\n",
    "\n",
    "tokenizer = Tokenizer(oov_token='unk')\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "maxlen = max_sentence_len\n",
    "# maxlen = 30\n",
    "\n",
    "seq_train = tokenizer.texts_to_sequences(x_train)\n",
    "seq_val = tokenizer.texts_to_sequences(x_val)\n",
    "seq_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "training_data = pad_sequences(seq_train, padding='post', maxlen=maxlen)\n",
    "val_data = pad_sequences(seq_val, padding='post', maxlen=maxlen)\n",
    "test_data = pad_sequences(seq_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, 100, input_length=max_sentence_len))\n",
    "model.add(layers.Conv1D(128,5))\n",
    "model.add(layers.MaxPooling1D(3,3, padding='same'))\n",
    "model.add(layers.Conv1D(64,5))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(layers.Dense(64, activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "# the output of evaluate:\n",
    "# [0.06994478800267066, 0.007670182166826462]\n",
    "# training loss: 0.0094\n",
    "# training acc: 0.0042\n",
    "# validation loss: 0.0503\n",
    "# validation acc: 0.0033"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"saved-a42-cnn.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='auto', period=1)\n",
    "\n",
    "history = model.fit(training_data, \n",
    "                    y_train,\n",
    "                    validation_data=(val_data, y_val),\n",
    "                    batch_size=30, \n",
    "                    epochs=100,\n",
    "                    callbacks=[checkpoint])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"saved-a42-cnn.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_scaled = mean_squared_error(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04641011154552032"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 261, 100)          1474300   \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 261, 256)          25856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 261, 256)          1024      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 261, 256)          0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 261, 64)           16448     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 261, 64)           0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 261, 1)            65        \n",
      "=================================================================\n",
      "Total params: 1,517,693\n",
      "Trainable params: 1,517,181\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, 100, input_length=max_sentence_len))\n",
    "model.add(Dense(256, input_dim=max_sentence_len,activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64,activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 30, 100)           1474300   \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 26, 128)           64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 9, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 5, 64)             41024     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                20544     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,600,061\n",
      "Trainable params: 1,600,061\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_size, 100, input_length=maxlen)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(3,3, padding='same'))\n",
    "model.add(layers.Conv1D(64,5))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(layers.Dense(64, activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.72671306],\n",
       "       [0.5388591 ],\n",
       "       [0.68009716],\n",
       "       ...,\n",
       "       [0.2591312 ],\n",
       "       [0.26152107],\n",
       "       [0.5186733 ]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
